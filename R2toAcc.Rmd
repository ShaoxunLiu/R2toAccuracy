---
title: "Evaluating continuous predicion models: R-square to clinical applications"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

## Introduction
A general goal in computational biology is to make prediction models. Those models range from genotype to phenotype predictions, structure to function predictions, health check indicators to disease risk assessment and others. 

The general goal of a prediction model is to reduce the number of biological tests that has to be done to identify a functional (disease causing) factor. For example, lets say there is 1 functional factor out of 100 candidates. Assuming experiments have 100% percent accuracy, someone who whats to identify the functional factor is expected to preform experiments on all 100 candidates to identify the functional factors. However, say there is a predictive model that is 70% accurate in predicting the functional factor, 7 out of 10 time, the experimenter would only need to test the candidates that are predicted to be positive. This saves time and money, and sometimes saves lives. 

The results of prediction models can come in two formats, either categorical or numerical, also known as continuous. For a categorical prediction model, the confusion table resulting from cross validation can provide a comprehensive representation of prediction accuracy, yet for continuous prediction models, an arbitrary threshold would be needed to transform continuous labels to categorical labels. To avoid making assumptions and maintain as much information as possible, a commonly accepted criteria to evaluate the accuracy of continuous prediction models is R-square values. 

R-square values shows the level of linearity between true and predicted labels where the closer it is to 1, the higher the linearity. Albeit high linearity being desirable, the practical value of a prediction model is to reduce the amount of in vivo/in vitro experiments to do before identifying the deciding factors (positive true labels). The problem is that R-square values do not tell us how much less experiments we can do. Here, I aim to build a framework to map the amount of test reduced to R-square values of the prediction model. This process will come in two step: first, mapping R-squared values to accuracy under any given threshold, and second, use Bayesian statistics to determine the number of test that can be reduced to achieve a given confidence level in identifying all positive values.   


## Part 1: R-square to Accuracy
A classic way of presenting prediction accuracy is through a confusion matrix and the accuracy can be calculated as: Accuracy = (True Positive + True negative) / (Total sample). Si mutation is used in this part to map R-square to accuracy.

create the functions that are used in the simulation
```{r}
#Get positive/negative label from predicted number with a given threshold
getLabel <- function(x, thresh = 0){
  if(x > thresh){
    return(1)
  }else{
    return(0)
  }
}
#Get Y value of a curve given X
getY <- function(dt, x, window = 10){
  dt$diff <- abs(dt$x-x)
  return(mean(dplyr::arrange(dt, diff)$y[1:window]))
}
#Get X value of a curve given Y
getX <- function(dt,y,window = 10){
  dt$diff <- abs(dt$y-y)
  return(mean(dplyr::arrange(dt, diff)$x[1:window]))
}
```

Simulate 10000 samples to be predicted and setting the proportion of positive labels as 0.3.This is also the prior distribution and will eventually be a variable in the final mapping function. To best simulate real-life circumstance, the true label of the samples follows a normal distribution. 
```{r}
#Simulate experiment
size <- 10000
sample <- rnorm(size)
probFalse <- 0.7
label <- sapply(sample, getLabel, thresh = qnorm(probFalse))
```

The process of predicting the true label is carried out by generating numbers as the predicted value for each sample. For each sample, the simulated prediction result is generated by modifying the true label with a random number sample from a normal distribution centered around 0 with standard deviation sd. By changing the value of sd, the linearity, measured by r-square, between the simulated true label and predicted label will decrease when sd gets higher. A function can be generalized between sd and R-square. To better distinguish between sds in the high R-square regions, 1/sd is used to show in the plot. 
```{r}
#Plot R^2 vs sd
sds <- c()
rsqs <- c()
for(i in 1:1000){
  mod <- i/100
  sd <- 1/mod
  est <- sample + rnorm(size)*sd
  dt <- data.frame(sample, est)
  lm <- summary(lm(est~sample,dt))
  rsq <- lm$r.squared
  sds <- c(sds, sd)
  rsqs <- c(rsqs, rsq)
}
mod <- 1/sds
plot(rsqs,mod, ylab = '1/sd', xlab = 'R-sqaure')
rsqVSmod <- data.frame(y=mod,x=rsqs)
```

With this function, we know that for a given R-square value, what sd value we would need to use to simulate the predicted results. Using the getLabel function, we are now able to get the accuracy measurement of a set of simulated true and predicted labels with a given sd that resembles a given R-sqaure. 
```{r}
#plot accuracy as a function of R^2
R2s <- c()
Accs <- c()
TPRs <- c()
TNRs <- c()
for(i in 1:1000){
  mod <- getY(rsqVSmod,i/1000)
  est <- sample + rnorm(size)/mod
  ExpLabel <- sapply(est,getLabel, thresh = qnorm(probFalse))
  table(ExpLabel, label)
  conf <- table(ExpLabel, label)
  acc <- sum(conf[2,2], conf[1,1])/sum(conf)
  TPR <- conf[2,2]/sum(conf[2,2], conf[1,2])
  TNR <- conf[1,1]/sum(conf[2,1], conf[1,1])
  R2s <- c(R2s, i/1000)
  Accs <- c(Accs, acc)
  TPRs <- c(TPRs, TPR)
  TNRs <- c(TNRs, TNR)
}
plot(R2s, Accs, xlab = 'R-square', ylab = 'Accuracy', ylim = c(0.5,1))
plot(R2s, TPRs, xlab = 'R-square', ylab = 'Sensitivity', ylim = c(0.5,1))
plot(R2s, TNRs, xlab = 'R-square', ylab = 'specificity', ylim = c(0.5,1))
AccsVSR2s <- data.frame(x=R2s, y=Accs)
```

From the simulated functions above, we can observe that sensitivity decreases faster as R-square drops when R-square is high, while specificity decreases faster when R-square is low. The Accuracy measurement can be regarded as an average between sensitivity and specificity. The above curve represents the function between R-square and accuracy. 

It is important to note that this simulation is based on the assumption that the prior distribution of positive labels are known. When the estimate of the prior distribution is unknown, Accuracy drops drastically. In the simulation below, the estimated prior distribution is 0.5 for the real prior distribution of 0.3. By including more predicted points as positive, is results in higher sensitivity at the expense of lower specificity. In general accuracy will drop in either cases of estimating for a higher or lower prior distribution, with higher estimation result in higher sensitivity and lower estimation resulting in higher specificity. In real-life applications, higher sensitivity with lower specificity will cause the examiner to have higher confidence in finding the functional factor in the positively predicted samples at the expense of performing more tests. 
```{r}
#plot accuracy as a function of R^2
R2s <- c()
Accs <- c()
TPRs <- c()
TNRs <- c()
for(i in 1:1000){
  mod <- getY(rsqVSmod,i/1000)
  est <- sample + rnorm(size)/mod
  ExpLabel <- sapply(est,getLabel, thresh = qnorm(0.5))
  table(ExpLabel, label)
  conf <- table(ExpLabel, label)
  acc <- sum(conf[2,2], conf[1,1])/sum(conf)
  TPR <- conf[2,2]/sum(conf[2,2], conf[1,2])
  TNR <- conf[1,1]/sum(conf[2,1], conf[1,1])
  R2s <- c(R2s, i/1000)
  Accs <- c(Accs, acc)
  TPRs <- c(TPRs, TPR)
  TNRs <- c(TNRs, TNR)
}
plot(R2s, Accs, xlab = 'R-square', ylab = 'Accuracy', ylim = c(0.5,1))
plot(R2s, TPRs, xlab = 'R-square', ylab = 'Sensitivity', ylim = c(0.5,1))
plot(R2s, TNRs, xlab = 'R-square', ylab = 'specificity', ylim = c(0.5,1))
AccsVSR2s <- data.frame(x=R2s, y=Accs)
```

This simulation can also be nested, as if the in vivo/in vitro experiment itself is not 100 percent accurate, we can use the R-square between multiple experimental trials of the same sample to estimate for the experimental accuracy. This can be generalized by adding a second 1/sd term to the simulated predicted label that resembles the experimental R-square. 
```{r}
#Assuming experimental R-sqaure is 0.9
mod_exp <- getY(rsqVSmod,0.9)
R2s <- c()
Accs <- c()
TPRs <- c()
TNRs <- c()
for(i in 1:1000){
  mod <- getY(rsqVSmod,i/1000)
  est_exp <- sample + rnorm(size)/mod_exp
  est <- est_exp + rnorm(size)/mod
  ExpLabel <- sapply(est,getLabel, thresh = qnorm(probFalse))
  table(ExpLabel, label)
  conf <- table(ExpLabel, label)
  acc <- sum(conf[2,2], conf[1,1])/sum(conf)
  TPR <- conf[2,2]/sum(conf[2,2], conf[1,2])
  TNR <- conf[1,1]/sum(conf[2,1], conf[1,1])
  R2s <- c(R2s, i/1000)
  Accs <- c(Accs, acc)
  TPRs <- c(TPRs, TPR)
  TNRs <- c(TNRs, TNR)
}
plot(R2s, Accs, xlab = 'R-square', ylab = 'Accuracy', ylim = c(0.5,1))
plot(R2s, TPRs, xlab = 'R-square', ylab = 'Sensitivity', ylim = c(0.5,1))
plot(R2s, TNRs, xlab = 'R-square', ylab = 'specificity', ylim = c(0.5,1))
AccsVSR2s <- data.frame(x=R2s, y=Accs)


```

# Part 2: Accuracy to number of tests to reduce
In theory, even if a prediction model gives predicted labels at a accuracy close to 100%, there always exists the possibility that the false negative is ranked last. As a results, it is never guaranteed that all positive samples can be cover unless all samples are tested. Thus, instead of saying how many samples to test to guarantee covering all positive samples, I will say how many samples to test to reach confidence level 0.95 in covering all positive samples. This translates to finding the number of samples that I can leave out before reaching false negative rate over 0.05. 

Given that we know the sensitivity and specificity of a predictive model, The false negative rate is 1 - sensitivity. 

At 0.95 confidence level, the number of samples that are false negatives = the number of samples to leave out * (1 - sensitivity)

This number should be less that total number of samples * positive rate * (1 - 0.95)

Thus, The maximum number of samples to leave out while being 95% confident that all positive samples are testes = total number of samples * positive rate * (1 - 0.95) / (1 - sensitivity)

This follows Bayes theorem as is needs an estimated prior distribution of positive rate, which is also needed in estimating the sensitivity of a prediction model given the R-square. This requires the estimate of prior distribution to be accurate for such calculation to be valid. 

Assuming an accurate estimation of prior distribution, The function of number of samples to leave out based on R-square value is simulated below. 
```{r}
confidenceLevel <- 0.95
R2s <- c()
TNRs <- c()
for(i in 1:1000){
  mod <- getY(rsqVSmod,i/1000)
  est <- sample + rnorm(size)/mod
  ExpLabel <- sapply(est,getLabel, thresh = qnorm(probFalse))
  table(ExpLabel, label)
  conf <- table(ExpLabel, label)
  acc <- sum(conf[2,2], conf[1,1])/sum(conf)
  TPR <- conf[2,2]/sum(conf[2,2], conf[1,2])
  TNR <- conf[1,1]/sum(conf[2,1], conf[1,1])
  R2s <- c(R2s, i/1000)
  TNRs <- c(TNRs, TNR)
}
leaveOut <- size * (1 - probFalse) * (1-confidenceLevel) / (1 - TNRs)
prop <- leaveOut/size
plot(R2s, prop, xlab = 'R-square', ylab = 'Proportion of samples to leave out')
AccsVSR2s <- data.frame(x=R2s, y=leaveOut)
```

Note that in this simulation workflow, the simulated curves are based on a set positive rate of 0.7, the numbers will change when the positive rate changes. As a whole, the proportion of samples to leave out is a function of true positive rate, estimated positive rate, confidence level, and R-square. 
